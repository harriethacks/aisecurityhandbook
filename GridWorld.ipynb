{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOU/SzRiWouGAq6huT6ah78"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Reinforcement learning with GridWorld"],"metadata":{"id":"_xo4FWO7XcnG"}},{"cell_type":"markdown","source":["This code defines a GridWorld environment, where an agent navigates a grid to reach a goal while avoiding a pit. It is useful for reinforcement learning (RL) experiments."],"metadata":{"id":"9GbzsTsYw6--"}},{"cell_type":"markdown","source":["We import our required packages and define the GridWorld class.\n","The GridWorld class defines how the world works. The agent starts at the top-left corner (0,0), and the goal is at the bottom-right corner with a pit one step before it. The agent moves up, down, left, or right, with boundaries preventing it from going outside the grid. Each move results in a reward or penalty: reaching the goal gives +10 points, falling into the pit gives -10 points, and every step incurs a -1 penalty. The reset() function returns the agent to the starting position, step(action) updates its position and returns the new state and reward, and render() prints a grid visualization showing the agent (A), goal (G), and pit (P). This setup can be used for reinforcement learning experiments like Q-learning."],"metadata":{"id":"0Zwz22S9w9at"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"bf28MVfxma5w","executionInfo":{"status":"ok","timestamp":1740950953025,"user_tz":0,"elapsed":84,"user":{"displayName":"Harriet","userId":"08169952984174915407"}}},"outputs":[],"source":["#Import the required packages\n","import numpy as np\n","import random\n","\n","#Define the GridWorld class\n","class GridWorld:\n","    def __init__(self, size=5):\n","        self.size = size\n","        self.state = (0, 0)  # Agent starts at top-left corner\n","        self.goal = (size - 1, size - 1)  # Goal at bottom-right corner\n","        self.pit = (size - 2, size - 2)  # Pit at one position before the goal\n","\n","    def reset(self):\n","        self.state = (0, 0)  # Reset agent to the starting position\n","        return self.state\n","\n","    def step(self, action):\n","        x, y = self.state\n","        # Define movement directions: [Up, Down, Left, Right]\n","        moves = [(-1, 0), (1, 0), (0, -1), (0, 1)]\n","        dx, dy = moves[action]\n","        new_state = (x + dx, y + dy)\n","\n","        # Stay within grid boundaries\n","        new_state = (max(0, min(self.size - 1, new_state[0])),\n","                     max(0, min(self.size - 1, new_state[1])))\n","\n","        self.state = new_state\n","\n","        # Define rewards\n","        if self.state == self.goal:\n","            return self.state, 10, True  # Goal reached\n","        elif self.state == self.pit:\n","            return self.state, -10, True  # Fell into the pit\n","        else:\n","            return self.state, -1, False  # Step penalty\n","\n","    def render(self):\n","        grid = np.zeros((self.size, self.size), dtype=str)\n","        grid[:] = '.'\n","        grid[self.goal] = 'G'\n","        grid[self.pit] = 'P'\n","        x, y = self.state\n","        grid[x, y] = 'A'\n","        print(\"\\n\".join(\" \".join(row) for row in grid))\n","        print()"]},{"cell_type":"markdown","source":["The QLearningAgent class implements a Q-learning algorithm for reinforcement learning in the GridWorld environment. It maintains a Q-table, a state-action matrix where each entry represents the agent's expected reward for taking a specific action in a given state. The agent balances exploration (choosing random actions) and exploitation (choosing the best-known action) using an epsilon-greedy strategy, where the exploration rate (epsilon) decays over time. The update_q_value method updates the Q-values using the Bellman equation, incorporating the learning rate (lr) and discount factor (gamma). The train method runs for multiple episodes, allowing the agent to learn by interacting with the environment, receiving rewards, and refining its Q-values. After training, the test method evaluates the agent's performance by making greedy (optimal) moves based on the learned Q-table, while visually rendering the environment. The model gradually improves as it learns the best path to reach the goal while avoiding pitfalls."],"metadata":{"id":"s64d1dlhx7MD"}},{"cell_type":"code","source":["#Define the QLearning functions\n","class QLearningAgent:\n","    def __init__(self, env, learning_rate=0.1, discount_factor=0.9, exploration_rate=1.0, exploration_decay=0.99):\n","        self.env = env\n","        self.q_table = np.zeros((env.size, env.size, 4))  # State-action table\n","        self.lr = learning_rate\n","        self.gamma = discount_factor\n","        self.epsilon = exploration_rate\n","        self.epsilon_decay = exploration_decay\n","\n","    def choose_action(self, state):\n","        if random.random() < self.epsilon:\n","            return random.randint(0, 3)  # Random action (exploration)\n","        else:\n","            x, y = state\n","            return np.argmax(self.q_table[x, y])  # Best action (exploitation)\n","\n","    def update_q_value(self, state, action, reward, next_state):\n","        x, y = state\n","        next_x, next_y = next_state\n","        old_value = self.q_table[x, y, action]\n","        next_max = np.max(self.q_table[next_x, next_y])\n","        # Update Q-value using the Bellman equation\n","        self.q_table[x, y, action] = old_value + self.lr * (reward + self.gamma * next_max - old_value)\n","\n","    def train(self, episodes=1000):\n","        for episode in range(episodes):\n","            state = self.env.reset()\n","            done = False\n","            while not done:\n","                action = self.choose_action(state)\n","                next_state, reward, done = self.env.step(action)\n","                self.update_q_value(state, action, reward, next_state)\n","                state = next_state\n","            # Decay exploration rate\n","            self.epsilon *= self.epsilon_decay\n","            if episode % 100 == 0:\n","                print(f\"Episode {episode}, Epsilon: {self.epsilon:.2f}\")\n","\n","    def test(self):\n","        state = self.env.reset()\n","        done = False\n","        self.env.render()\n","        while not done:\n","            action = np.argmax(self.q_table[state[0], state[1]])\n","            state, reward, done = self.env.step(action)\n","            self.env.render()\n"],"metadata":{"id":"zCpaCh9EqTlI","executionInfo":{"status":"ok","timestamp":1740950953039,"user_tz":0,"elapsed":19,"user":{"displayName":"Harriet","userId":"08169952984174915407"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["The agent.train(episodes=1000) function runs the training process for 1,000 episodes, during which the agent repeatedly explores the environment, updates its Q-values using the Bellman equation, and gradually improves its decision-making. As training progresses, the agent reduces exploration (epsilon decay) and shifts towards exploiting its learned policy to maximize rewards. After training, the agent will have learned an optimal path to the goal while minimizing penalties."],"metadata":{"id":"mmipIVN3yI-a"}},{"cell_type":"code","source":["# Initialize the environment and agent\n","env = GridWorld(size=5)\n","agent = QLearningAgent(env)\n","\n","# Train the agent\n","print(\"Training the agent...\")\n","agent.train(episodes=1000)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Fckg7eJ3qeO6","executionInfo":{"status":"ok","timestamp":1740950953505,"user_tz":0,"elapsed":461,"user":{"displayName":"Harriet","userId":"08169952984174915407"}},"outputId":"707506f5-7987-4038-9081-29047f1c55bf"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Training the agent...\n","Episode 0, Epsilon: 0.99\n","Episode 100, Epsilon: 0.36\n","Episode 200, Epsilon: 0.13\n","Episode 300, Epsilon: 0.05\n","Episode 400, Epsilon: 0.02\n","Episode 500, Epsilon: 0.01\n","Episode 600, Epsilon: 0.00\n","Episode 700, Epsilon: 0.00\n","Episode 800, Epsilon: 0.00\n","Episode 900, Epsilon: 0.00\n"]}]},{"cell_type":"markdown","source":["This code tests the trained Q-learning agent to see if it has successfully learned how to navigate the GridWorld environment."],"metadata":{"id":"CyVD-Iw9yYwn"}},{"cell_type":"code","source":["# Let's test the agent and see if it can make it to the end\n","print(\"Testing the agent...\")\n","agent.test()"],"metadata":{"id":"qsFiFtpLXiz-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1740950953507,"user_tz":0,"elapsed":24,"user":{"displayName":"Harriet","userId":"08169952984174915407"}},"outputId":"f659a9ae-1f22-479b-95c5-d326421c9005"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Testing the agent...\n","A . . . .\n",". . . . .\n",". . . . .\n",". . . P .\n",". . . . G\n","\n",". . . . .\n","A . . . .\n",". . . . .\n",". . . P .\n",". . . . G\n","\n",". . . . .\n",". A . . .\n",". . . . .\n",". . . P .\n",". . . . G\n","\n",". . . . .\n",". . A . .\n",". . . . .\n",". . . P .\n",". . . . G\n","\n",". . . . .\n",". . . . .\n",". . A . .\n",". . . P .\n",". . . . G\n","\n",". . . . .\n",". . . . .\n",". . . A .\n",". . . P .\n",". . . . G\n","\n",". . . . .\n",". . . . .\n",". . . . A\n",". . . P .\n",". . . . G\n","\n",". . . . .\n",". . . . .\n",". . . . .\n",". . . P A\n",". . . . G\n","\n",". . . . .\n",". . . . .\n",". . . . .\n",". . . P .\n",". . . . A\n","\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"LBIzFNrdqhWZ","executionInfo":{"status":"ok","timestamp":1740950953507,"user_tz":0,"elapsed":4,"user":{"displayName":"Harriet","userId":"08169952984174915407"}}},"execution_count":4,"outputs":[]}]}